{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RW-LWJ2gbrh",
    "tags": []
   },
   "source": [
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "\n",
    "\n",
    "# Mountain Car\n",
    "***\n",
    "\n",
    "Main reference: [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jBelL8nBZVsn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/lucarinna/anaconda3/envs/NNDL/lib/python3.9/site-packages (0.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/lucarinna/anaconda3/envs/NNDL/lib/python3.9/site-packages (from gym) (2.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/lucarinna/anaconda3/envs/NNDL/lib/python3.9/site-packages (from gym) (4.10.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/lucarinna/anaconda3/envs/NNDL/lib/python3.9/site-packages (from gym) (0.0.7)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/lucarinna/anaconda3/envs/NNDL/lib/python3.9/site-packages (from gym) (1.21.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/lucarinna/anaconda3/envs/NNDL/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a7EkHQ0VsNnJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucarinna/anaconda3/envs/NNDL/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from collections import deque # this python module implements exactly what we need for the replay memeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0HIwpatSteb"
   },
   "outputs": [],
   "source": [
    "!apt update\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install piglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxM-vHKz1r_M"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0oXbzI2ZAfv"
   },
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yr5j5rz0ZEJf"
   },
   "outputs": [],
   "source": [
    "# This code creates a virtual display to draw game images on. \n",
    "# If you are running locally, just ignore it\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StaxWEA0Z-O1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_videos():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  mp4list.sort()\n",
    "  for mp4 in mp4list:\n",
    "    print(f\"\\nSHOWING VIDEO {mp4}\")\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    \n",
    "def wrap_env(env, video_callable=None):\n",
    "  env = Monitor(env, './video', force=True, video_callable=video_callable)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wA_0gAmJfFlT"
   },
   "source": [
    "## Experience replay (Replay Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqPUYNrpZhPw"
   },
   "source": [
    "\"*To perform experience replay, we store the agent's experiences e_t=(s_t,a_t,r_t,s_t+1) at each time-step t in a data set D_t={e_1,…,e_t}. During learning, we apply Q-learning updates, on samples (or mini-batches) of experience (s,a,r,s′)∼U(D), drawn uniformly at random from the pool of stored samples.*\"\n",
    "\n",
    "Source: https://www.nature.com/articles/nature14236\n",
    "\n",
    "In practice, what we need is a queue with a predefined capacity. When we reach the maximum capacity, the oldest element in the queue will be replaced with the new one.\n",
    "\n",
    "This exact behavior is implemented by the `deque` object from the python `collections` library (https://docs.python.org/3/library/collections.html#collections.deque):\n",
    "\n",
    "\"*If maxlen is not specified or is None, deques may grow to an arbitrary length. Otherwise, the deque is bounded to the specified maximum length. Once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end.*\"\n",
    "\n",
    "The random sampling can be easily achieved using the `random.sample` method (https://docs.python.org/3/library/random.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1axKWEBl8uik"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
    "        return random.sample(self.memory, batch_size) # Randomly select \"batch_size\" samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory) # Return the number of samples currently stored in the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74v3ChPPqox3"
   },
   "source": [
    "Test if it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mtwQjNqjaT4E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT MEMORY SIZE: 0\n",
      "CURRENT MEMORY SIZE: 1\n",
      "CURRENT MEMORY SIZE: 2\n",
      "CURRENT MEMORY SIZE: 3\n",
      "CURRENT MEMORY SIZE: 3\n",
      "CURRENT MEMORY SIZE: 3\n",
      "\n",
      "CONTENT OF THE MEMORY\n",
      "deque([(3, 3, 3, 3), (4, 4, 4, 4), (5, 5, 5, 5)], maxlen=3)\n",
      "\n",
      "RANDOM SAMPLING\n",
      "[(3, 3, 3, 3), (5, 5, 5, 5)]\n",
      "[(4, 4, 4, 4), (3, 3, 3, 3)]\n",
      "[(4, 4, 4, 4), (3, 3, 3, 3)]\n",
      "[(3, 3, 3, 3), (4, 4, 4, 4)]\n",
      "[(3, 3, 3, 3), (5, 5, 5, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Define the replay memory\n",
    "replay_mem = ReplayMemory(capacity=3)\n",
    "\n",
    "# Push some samples\n",
    "print(f\"CURRENT MEMORY SIZE: {len(replay_mem)}\")\n",
    "replay_mem.push(1,1,1,1)\n",
    "print(f\"CURRENT MEMORY SIZE: {len(replay_mem)}\")\n",
    "replay_mem.push(2,2,2,2)\n",
    "print(f\"CURRENT MEMORY SIZE: {len(replay_mem)}\")\n",
    "replay_mem.push(3,3,3,3)\n",
    "print(f\"CURRENT MEMORY SIZE: {len(replay_mem)}\")\n",
    "replay_mem.push(4,4,4,4)\n",
    "print(f\"CURRENT MEMORY SIZE: {len(replay_mem)}\")\n",
    "replay_mem.push(5,5,5,5)\n",
    "print(f\"CURRENT MEMORY SIZE: {len(replay_mem)}\")\n",
    "\n",
    "# Check the content of the memory\n",
    "print('\\nCONTENT OF THE MEMORY')\n",
    "print(replay_mem.memory)\n",
    "\n",
    "# Random sample\n",
    "print('\\nRANDOM SAMPLING')\n",
    "for i in range(5):\n",
    "    print(replay_mem.sample(2)) # Select 2 samples randomly from the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-nBdx6KfKNE"
   },
   "source": [
    "## Policy network\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt2uYYCSrL8q"
   },
   "source": [
    "The policy network takes a state as input, and provides the Q-value for each of the possible actions.\n",
    "\n",
    "Let's define a simple generic fully-connected feed forward network with `state_space_dim` inputs and `action_space_dim` outputs (e.g. 2 hidden layers with 128 neurons each) with an activation function (e.g. Tanh). \n",
    "\n",
    "Be sure to keep a linear output activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rdsRx6wfZw1"
   },
   "source": [
    "## Network definition\n",
    "\n",
    "Also for the Mountain Car game I exploit the very same policy Network defined in Lab.07 for the Cartpole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W41rXekb8x7K"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_space_dim, action_space_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(state_space_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, action_space_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tLPAsTT4gpeW"
   },
   "outputs": [],
   "source": [
    "# Define an example network\n",
    "net = DQN(state_space_dim=2, action_space_dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynfEXqjGfbpl"
   },
   "source": [
    "## Exploration Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1Hs7GrEw5FW"
   },
   "source": [
    "Starting from the estimated Q-values (one for each action), we need to choose the proper action. This action may be the one expected to provide the highest long term reward (exploitation), or maybe we want to find a better policy by choosing a different action (exploration).\n",
    "\n",
    "The exploration policy controls this behavior, typically by varying a single parameter.\n",
    "\n",
    "Since our Q-values estimates are far from the true values at the beginning of the training, a high exploration is preferred in the initial phase.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "`Current state -> Policy network -> Q-values -> Exploration Policy -> Action`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x-kRJbQfqBR"
   },
   "source": [
    "### Epsilon-greedy policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFVPF0aMy3be"
   },
   "source": [
    "With an epsilon-greedy policy we choose a **non optimal** action with probability epsilon, otherwise choose the best action (the one corresponding to the highest Q-value).\n",
    "\n",
    "NOTE: there is a difference wrt paper in that they do not exclude the best action when chosing the action at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZXfh_Ub1fv4c"
   },
   "outputs": [],
   "source": [
    "def choose_action_epsilon_greedy(net, state, epsilon):\n",
    "    \n",
    "    if epsilon > 1 or epsilon < 0:\n",
    "        raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state, dtype=torch.float32) # Convert the state to tensor\n",
    "        net_out = net(state)\n",
    "\n",
    "    # Get the best action (argmax of the network output)\n",
    "    best_action = int(net_out.argmax())\n",
    "    # Get the number of possible actions\n",
    "    action_space_dim = net_out.shape[-1]\n",
    "\n",
    "    # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "    if random.random() < epsilon:\n",
    "        # List of non-optimal actions\n",
    "        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "        # Select randomly\n",
    "        action = random.choice(non_optimal_actions)\n",
    "    else:\n",
    "        # Select best action\n",
    "        action = best_action\n",
    "        \n",
    "    return action, net_out.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nklFJdMoiDVo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION: 1\n",
      "Q-VALUES: [-7.2494149e-06  6.9364361e-02 -1.0163989e-02]\n"
     ]
    }
   ],
   "source": [
    "# Test if it works as expected\n",
    "state = (0, 0)\n",
    "epsilon = 0.5\n",
    "chosen_action, q_values = choose_action_epsilon_greedy(net, state, epsilon)\n",
    "\n",
    "print(f\"ACTION: {chosen_action}\")\n",
    "print(f\"Q-VALUES: {q_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vvxi7b82f0g4"
   },
   "source": [
    "### Softmax policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I0r6m-p1UQG"
   },
   "source": [
    "With a softmax policy we choose the action based on a distribution obtained applying a softmax (with temperature $\\tau$) to the estimated Q-values. The highest the temperature, the more the distribution will converge to a random uniform distribution. At zero temperature, instead, the policy will always choose the action with the highest Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "taW_cjBsf4sW"
   },
   "outputs": [],
   "source": [
    "def choose_action_softmax(net, state, temperature):\n",
    "    \n",
    "    if temperature < 0:\n",
    "        raise Exception('The temperature value must be greater than or equal to 0 ')\n",
    "        \n",
    "    # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
    "    if temperature == 0:\n",
    "        return choose_action_epsilon_greedy(net, state, 0)\n",
    "    \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        net_out = net(state)\n",
    "\n",
    "    # Apply softmax with temp\n",
    "    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
    "    softmax_out = nn.functional.softmax(net_out / temperature, dim=0).numpy()\n",
    "                \n",
    "    # Sample the action using softmax output as mass pdf\n",
    "    all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
    "    action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n",
    "    \n",
    "    return action, net_out.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CAIMqC5kldtB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION: 1\n",
      "Q-VALUES: [-7.2494149e-06  6.9364361e-02 -1.0163989e-02]\n"
     ]
    }
   ],
   "source": [
    "state = (0, 0)\n",
    "temperature = 1\n",
    "chosen_action, q_values = choose_action_softmax(net, state, temperature)\n",
    "\n",
    "print(f\"ACTION: {chosen_action}\")\n",
    "print(f\"Q-VALUES: {q_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rOvTpcbMPnR"
   },
   "source": [
    "### Exploration profile\n",
    "\n",
    "Let's consider, for example, an exponentially decreasing exploration profile using a softmax policy.\n",
    "\n",
    "$$\n",
    "\\text{softmax_temperature}  = \\text{initial_temperature} * \\text{exp_decay}^i \\qquad \\text{for $i$ = 1, 2, ..., num_iterations } \n",
    "$$\n",
    "\n",
    "Alternatively, you can consider an epsilon greedy policy. In this case the exploration would be controlled by the epsilon parameter, for which you should consider a different initial value (max 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "G-f78CC3ptt2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Exploration profile (Softmax temperature)')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Define exploration profile\n",
    "initial_value = 5\n",
    "num_iterations = 500\n",
    "exp_decay = np.exp(-np.log(initial_value) / num_iterations * 5) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations\n",
    "exploration_profile = [initial_value * (exp_decay ** i) for i in range(num_iterations)]\n",
    "\n",
    "### Plot exploration profile\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(exploration_profile)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Exploration profile (Softmax temperature)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC_cbOXjEJlb"
   },
   "source": [
    "# Mountain Car (MountainCar-v0)\n",
    "\n",
    "The car is placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. There's gravity but no friction. The goal is to strategically accelerate the car to reach the goal state on top of the right hill.\n",
    "https://www.gymlibrary.ml/environments/classic_control/mountain_car/\n",
    "\n",
    "#### Observation space\n",
    "| Num | Observation | Min | Max | Unit |\n",
    "|:--|:--|:--|:--|:--|\n",
    "|0|position of the car along the x-axis|-Inf|+Inf|postion (m)|\n",
    "|1|velocity of the car|-Inf|+Inf|postion (m)|\n",
    "\n",
    "#### Action space\n",
    "| Num | Observation | Value | Unit |\n",
    "|:--|:--|:--|:--|\n",
    "|0|Accelerate to the left|Inf|postion (m)|\n",
    "|1|Don't accelerate |Inf|postion (m)|\n",
    "|2|Accelerate to the right|Inf|postion (m)|\n",
    "\n",
    "The position is clipped to the range [-1.2, 0.6] and the velocity is clipped to the range [-0.07, 0.07].\n",
    "The goal is to reach the flag placed on top of the right hill as quick as possible, as such the agent is penalised with a reward of -1 for each timestep.\n",
    "\n",
    "The episode terminates is either of the following happens:\n",
    "* The position of the car is greater or equal than 0.5\n",
    "* The enght of the eposide is 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Ar0yNaNrCnjn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE SPACE SIZE: 2\n",
      "ACTION SPACE SIZE: 3\n"
     ]
    }
   ],
   "source": [
    "### Create environment\n",
    "env = gym.make('MountainCar-v0') # Initialize the Gym environment\n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# Get the shapes of the state space (observation_space) and action space (action_space)\n",
    "state_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.n\n",
    "\n",
    "print(f\"STATE SPACE SIZE: {state_space_dim}\")\n",
    "print(f\"ACTION SPACE SIZE: {action_space_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GR0X30SQwwC7"
   },
   "source": [
    "## Random agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJP0ggFyOLLG"
   },
   "source": [
    "First of all, to check that the environment is working properly, let's try with an agent which simply choose an action randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "2g4NatpgSpbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - FINAL SCORE: -200.0\n",
      "EPISODE 2 - FINAL SCORE: -200.0\n",
      "EPISODE 3 - FINAL SCORE: -200.0\n",
      "EPISODE 4 - FINAL SCORE: -200.0\n",
      "EPISODE 5 - FINAL SCORE: -200.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Gym environment\n",
    "env = gym.make('MountainCar-v0') \n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "\n",
    "# Let's try for a total of 10 episodes\n",
    "for num_episode in range(5): \n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "      # Choose a random action\n",
    "      action = random.choice([0, 1, 2])\n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      # Visually render the environment (optional, comment this line to speed up the simulation)\n",
    "      env.render()\n",
    "      # Update the final score \n",
    "      score += reward \n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "    # Print the final score\n",
    "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KXpjzf2vdeL"
   },
   "source": [
    "# Network update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ3P9MKjxCAL"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7gSKTn-ymDZ"
   },
   "source": [
    "In this case we will use the Huber loss as loss function (https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html). The Huber loss uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It is less sensitive to outliers than the MSELoss and in some cases prevents exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "lVpk-g0i9d-B"
   },
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "### PARAMETERS\n",
    "gamma = 0.97   # gamma parameter for the long term reward\n",
    "replay_memory_capacity = 10000   # Replay memory capacity\n",
    "lr = 1e-1   # Optimizer learning rate\n",
    "target_net_update_steps = 10   # Number of episodes to wait before updating the target network\n",
    "batch_size = 128   # Number of samples to take from the replay memory for each update\n",
    "bad_state_penalty = 0   # Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n",
    "min_samples_for_training = 1000   # Minimum samples in the replay memory to enable the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "2k3ZQuh8xHo7"
   },
   "outputs": [],
   "source": [
    "### Initialize the replay memory\n",
    "replay_mem = ReplayMemory(replay_memory_capacity)    \n",
    "\n",
    "### Initialize the policy network\n",
    "policy_net = DQN(state_space_dim, action_space_dim)\n",
    "\n",
    "### Initialize the target network with the same weights of the policy network\n",
    "target_net = DQN(state_space_dim, action_space_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "\n",
    "### Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n",
    "\n",
    "### Initialize the loss function (Huber loss)\n",
    "loss_fn = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQUTrt8DzJXO"
   },
   "source": [
    "## Update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Sj1hEvPOvkBX"
   },
   "outputs": [],
   "source": [
    "def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n",
    "        \n",
    "    # Sample the data from the replay memory\n",
    "    batch = replay_mem.sample(batch_size)\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # Create tensors for each element of the batch\n",
    "    states      = torch.tensor(np.array([s[0] for s in batch]), dtype=torch.float32)\n",
    "    actions     = torch.tensor(np.array([s[1] for s in batch]), dtype=torch.int64)\n",
    "    rewards     = torch.tensor(np.array([s[3] for s in batch]), dtype=torch.float32)\n",
    "\n",
    "    # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "    non_final_next_states = torch.tensor(np.array([s[2] for s in batch if s[2] is not None]), dtype=torch.float32) # the next state can be None if the game has ended\n",
    "    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n",
    "\n",
    "    # Compute all the Q values (forward pass)\n",
    "    policy_net.train()\n",
    "    q_values = policy_net(states)\n",
    "    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "    with torch.no_grad():\n",
    "        target_net.eval()\n",
    "        q_values_target = target_net(non_final_next_states)\n",
    "    next_state_max_q_values = torch.zeros(batch_size)\n",
    "    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "    # Compute the Huber loss\n",
    "    loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Ab0RMB5CCC"
   },
   "source": [
    "## Training loop\n",
    "***\n",
    "\n",
    "After several trials I realized that using the standard reward, the agent could not beat the game within the 200 steps.\n",
    "This could be related to the fact that before the Network actually starts to learn how to win, the agent needs to explore a lot of different random actions.\n",
    "Due to computaion constraints I could not perform such a long training with a huge exploration phase.\n",
    "\n",
    "In order to solve this issue I modified the reward and I found out, after several trials that the best reward consist into:\n",
    "*  -1 at each step\n",
    "* +20 whenever the car reaches a new maximum displamecemnt from the valley\n",
    "* +20 whenever the car reaches a new maximum velocity\n",
    "* +2100 -(steps\\*10) when the car reach or surpass 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "WF6Zf53FoRDZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating target network...\n",
      "EPISODE: 1 - FINAL SCORE: -200 - Temperature: 5.0\n",
      "EPISODE: 2 - FINAL SCORE: -200 - Temperature: 4.920172216817288\n",
      "EPISODE: 3 - FINAL SCORE: -200 - Temperature: 4.841618928628149\n",
      "EPISODE: 4 - FINAL SCORE: -200 - Temperature: 4.76431978741058\n",
      "EPISODE: 5 - FINAL SCORE: -200 - Temperature: 4.688254770010077\n",
      "EPISODE: 6 - FINAL SCORE: -200 - Temperature: 4.613404172952941\n",
      "EPISODE: 7 - FINAL SCORE: -200 - Temperature: 4.5397486073424\n",
      "EPISODE: 8 - FINAL SCORE: -200 - Temperature: 4.46726899383621\n",
      "EPISODE: 9 - FINAL SCORE: -200 - Temperature: 4.395946557704448\n",
      "EPISODE: 10 - FINAL SCORE: -200 - Temperature: 4.325762823966204\n",
      "Updating target network...\n",
      "EPISODE: 11 - FINAL SCORE: -200 - Temperature: 4.256699612603922\n",
      "EPISODE: 12 - FINAL SCORE: -200 - Temperature: 4.188739033854146\n",
      "EPISODE: 13 - FINAL SCORE: -200 - Temperature: 4.121863483573451\n",
      "EPISODE: 14 - FINAL SCORE: -200 - Temperature: 4.056055638678363\n",
      "EPISODE: 15 - FINAL SCORE: -200 - Temperature: 3.991298452658077\n",
      "EPISODE: 16 - FINAL SCORE: -200 - Temperature: 3.92757515115882\n",
      "EPISODE: 17 - FINAL SCORE: -200 - Temperature: 3.8648692276387173\n",
      "EPISODE: 18 - FINAL SCORE: -200 - Temperature: 3.8031644390920216\n",
      "EPISODE: 19 - FINAL SCORE: -200 - Temperature: 3.742444801841614\n",
      "EPISODE: 20 - FINAL SCORE: -200 - Temperature: 3.682694587398678\n",
      "Updating target network...\n",
      "EPISODE: 21 - FINAL SCORE: -200 - Temperature: 3.623898318388476\n",
      "EPISODE: 22 - FINAL SCORE: -200 - Temperature: 3.566040764541174\n",
      "EPISODE: 23 - FINAL SCORE: -200 - Temperature: 3.509106938746673\n",
      "EPISODE: 24 - FINAL SCORE: -200 - Temperature: 3.4530820931724286\n",
      "EPISODE: 25 - FINAL SCORE: -200 - Temperature: 3.397951715443254\n",
      "EPISODE: 26 - FINAL SCORE: -200 - Temperature: 3.343701524882108\n",
      "EPISODE: 27 - FINAL SCORE: -200 - Temperature: 3.290317468810909\n",
      "EPISODE: 28 - FINAL SCORE: -200 - Temperature: 3.2377857189104042\n",
      "EPISODE: 29 - FINAL SCORE: -200 - Temperature: 3.186092667638152\n",
      "EPISODE: 30 - FINAL SCORE: -200 - Temperature: 3.135224924703702\n",
      "Updating target network...\n",
      "EPISODE: 31 - FINAL SCORE: -200 - Temperature: 3.085169313600046\n",
      "EPISODE: 32 - FINAL SCORE: -200 - Temperature: 3.035912868190442\n",
      "EPISODE: 33 - FINAL SCORE: -200 - Temperature: 2.987442829349739\n",
      "EPISODE: 34 - FINAL SCORE: -200 - Temperature: 2.9397466416593234\n",
      "EPISODE: 35 - FINAL SCORE: -200 - Temperature: 2.892811950154826\n",
      "EPISODE: 36 - FINAL SCORE: -200 - Temperature: 2.8466265971257627\n",
      "EPISODE: 37 - FINAL SCORE: -200 - Temperature: 2.801178618966263\n",
      "EPISODE: 38 - FINAL SCORE: -200 - Temperature: 2.7564562430760855\n",
      "EPISODE: 39 - FINAL SCORE: -200 - Temperature: 2.7124478848111035\n",
      "EPISODE: 40 - FINAL SCORE: -200 - Temperature: 2.669142144482482\n",
      "Updating target network...\n",
      "EPISODE: 41 - FINAL SCORE: -200 - Temperature: 2.6265278044037648\n",
      "EPISODE: 42 - FINAL SCORE: -200 - Temperature: 2.5845938259851033\n",
      "EPISODE: 43 - FINAL SCORE: -200 - Temperature: 2.54332934687388\n",
      "EPISODE: 44 - FINAL SCORE: -200 - Temperature: 2.5027236781409847\n",
      "EPISODE: 45 - FINAL SCORE: -200 - Temperature: 2.462766301512009\n",
      "EPISODE: 46 - FINAL SCORE: -200 - Temperature: 2.4234468666426507\n",
      "EPISODE: 47 - FINAL SCORE: -200 - Temperature: 2.384755188437616\n",
      "EPISODE: 48 - FINAL SCORE: -200 - Temperature: 2.346681244412327\n",
      "EPISODE: 49 - FINAL SCORE: -200 - Temperature: 2.30921517209675\n",
      "EPISODE: 50 - FINAL SCORE: -200 - Temperature: 2.2723472664806765\n",
      "Updating target network...\n",
      "EPISODE: 51 - FINAL SCORE: -200 - Temperature: 2.2360679774997867\n",
      "EPISODE: 52 - FINAL SCORE: -200 - Temperature: 2.200367907561855\n",
      "EPISODE: 53 - FINAL SCORE: -200 - Temperature: 2.1652378091124462\n",
      "EPISODE: 54 - FINAL SCORE: -200 - Temperature: 2.1306685822394784\n",
      "EPISODE: 55 - FINAL SCORE: -200 - Temperature: 2.0966512723160324\n",
      "EPISODE: 56 - FINAL SCORE: -200 - Temperature: 2.063177067680792\n",
      "EPISODE: 57 - FINAL SCORE: -200 - Temperature: 2.030237297355519\n",
      "EPISODE: 58 - FINAL SCORE: -200 - Temperature: 1.9978234287989685\n",
      "EPISODE: 59 - FINAL SCORE: -200 - Temperature: 1.965927065696667\n",
      "EPISODE: 60 - FINAL SCORE: -200 - Temperature: 1.9345399457859753\n",
      "Updating target network...\n",
      "EPISODE: 61 - FINAL SCORE: -200 - Temperature: 1.9036539387158757\n",
      "EPISODE: 62 - FINAL SCORE: -200 - Temperature: 1.8732610439409303\n",
      "EPISODE: 63 - FINAL SCORE: -200 - Temperature: 1.843353388648863\n",
      "EPISODE: 64 - FINAL SCORE: -200 - Temperature: 1.813923225721227\n",
      "EPISODE: 65 - FINAL SCORE: -200 - Temperature: 1.784962931726635\n",
      "EPISODE: 66 - FINAL SCORE: -200 - Temperature: 1.7564650049460244\n",
      "EPISODE: 67 - FINAL SCORE: -200 - Temperature: 1.7284220634294543\n",
      "EPISODE: 68 - FINAL SCORE: -200 - Temperature: 1.7008268430839215\n",
      "EPISODE: 69 - FINAL SCORE: -200 - Temperature: 1.6736721957917136\n",
      "EPISODE: 70 - FINAL SCORE: -200 - Temperature: 1.6469510875587947\n",
      "Updating target network...\n",
      "EPISODE: 71 - FINAL SCORE: -200 - Temperature: 1.6206565966927597\n",
      "EPISODE: 72 - FINAL SCORE: -200 - Temperature: 1.5947819120098752\n",
      "EPISODE: 73 - FINAL SCORE: -200 - Temperature: 1.5693203310707482\n",
      "EPISODE: 74 - FINAL SCORE: -200 - Temperature: 1.5442652584441607\n",
      "EPISODE: 75 - FINAL SCORE: -200 - Temperature: 1.5196102039986257\n",
      "EPISODE: 76 - FINAL SCORE: -200 - Temperature: 1.4953487812212176\n",
      "EPISODE: 77 - FINAL SCORE: -200 - Temperature: 1.4714747055632458\n",
      "EPISODE: 78 - FINAL SCORE: -200 - Temperature: 1.4479817928123362\n",
      "EPISODE: 79 - FINAL SCORE: -200 - Temperature: 1.4248639574905086\n",
      "EPISODE: 80 - FINAL SCORE: -200 - Temperature: 1.402115211277826\n",
      "Updating target network...\n",
      "EPISODE: 81 - FINAL SCORE: -200 - Temperature: 1.379729661461212\n",
      "EPISODE: 82 - FINAL SCORE: -200 - Temperature: 1.3577015094080356\n",
      "EPISODE: 83 - FINAL SCORE: -200 - Temperature: 1.3360250490640624\n",
      "EPISODE: 84 - FINAL SCORE: -200 - Temperature: 1.3146946654753908\n",
      "EPISODE: 85 - FINAL SCORE: -200 - Temperature: 1.2937048333339831\n",
      "EPISODE: 86 - FINAL SCORE: -200 - Temperature: 1.273050115546421\n",
      "EPISODE: 87 - FINAL SCORE: -200 - Temperature: 1.2527251618255075\n",
      "EPISODE: 88 - FINAL SCORE: -200 - Temperature: 1.2327247073043606\n",
      "EPISODE: 89 - FINAL SCORE: -200 - Temperature: 1.2130435711726277\n",
      "EPISODE: 90 - FINAL SCORE: -200 - Temperature: 1.1936766553344775\n",
      "Updating target network...\n",
      "EPISODE: 91 - FINAL SCORE: -200 - Temperature: 1.1746189430880163\n",
      "EPISODE: 92 - FINAL SCORE: -200 - Temperature: 1.155865497825789\n",
      "EPISODE: 93 - FINAL SCORE: -200 - Temperature: 1.1374114617560263\n",
      "EPISODE: 94 - FINAL SCORE: -200 - Temperature: 1.1192520546443077\n",
      "EPISODE: 95 - FINAL SCORE: -200 - Temperature: 1.1013825725753177\n",
      "EPISODE: 96 - FINAL SCORE: -200 - Temperature: 1.0837983867343657\n",
      "EPISODE: 97 - FINAL SCORE: -200 - Temperature: 1.0664949422083647\n",
      "EPISODE: 98 - FINAL SCORE: -200 - Temperature: 1.049467756805951\n",
      "EPISODE: 99 - FINAL SCORE: -200 - Temperature: 1.0327124198964404\n",
      "EPISODE: 100 - FINAL SCORE: -200 - Temperature: 1.0162245912673231\n",
      "Updating target network...\n",
      "EPISODE: 101 - FINAL SCORE: -200 - Temperature: 0.9999999999999976\n",
      "EPISODE: 102 - FINAL SCORE: -200 - Temperature: 0.9840344433634551\n",
      "EPISODE: 103 - FINAL SCORE: -200 - Temperature: 0.9683237857256275\n",
      "EPISODE: 104 - FINAL SCORE: -200 - Temperature: 0.9528639574821138\n",
      "EPISODE: 105 - FINAL SCORE: -200 - Temperature: 0.937650954002013\n",
      "EPISODE: 106 - FINAL SCORE: -200 - Temperature: 0.9226808345905858\n",
      "EPISODE: 107 - FINAL SCORE: -200 - Temperature: 0.9079497214684777\n",
      "EPISODE: 108 - FINAL SCORE: -200 - Temperature: 0.8934537987672397\n",
      "EPISODE: 109 - FINAL SCORE: -200 - Temperature: 0.8791893115408875\n",
      "EPISODE: 110 - FINAL SCORE: -200 - Temperature: 0.8651525647932388\n",
      "Updating target network...\n",
      "EPISODE: 111 - FINAL SCORE: -183 - Temperature: 0.8513399225207823\n",
      "EPISODE: 112 - FINAL SCORE: -200 - Temperature: 0.837747806770827\n",
      "EPISODE: 113 - FINAL SCORE: -182 - Temperature: 0.8243726967146882\n",
      "EPISODE: 114 - FINAL SCORE: -200 - Temperature: 0.8112111277356706\n",
      "EPISODE: 115 - FINAL SCORE: -173 - Temperature: 0.7982596905316133\n",
      "EPISODE: 116 - FINAL SCORE: -200 - Temperature: 0.7855150302317621\n",
      "EPISODE: 117 - FINAL SCORE: -175 - Temperature: 0.7729738455277415\n",
      "EPISODE: 118 - FINAL SCORE: -187 - Temperature: 0.7606328878184023\n",
      "EPISODE: 119 - FINAL SCORE: -177 - Temperature: 0.7484889603683209\n",
      "EPISODE: 120 - FINAL SCORE: -200 - Temperature: 0.7365389174797337\n",
      "Updating target network...\n",
      "EPISODE: 121 - FINAL SCORE: -178 - Temperature: 0.7247796636776933\n",
      "EPISODE: 122 - FINAL SCORE: -183 - Temperature: 0.7132081529082329\n",
      "EPISODE: 123 - FINAL SCORE: -181 - Temperature: 0.7018213877493328\n",
      "EPISODE: 124 - FINAL SCORE: -171 - Temperature: 0.6906164186344841\n",
      "EPISODE: 125 - FINAL SCORE: -177 - Temperature: 0.6795903430886491\n",
      "EPISODE: 126 - FINAL SCORE: -178 - Temperature: 0.66874030497642\n",
      "EPISODE: 127 - FINAL SCORE: -170 - Temperature: 0.6580634937621803\n",
      "EPISODE: 128 - FINAL SCORE: -170 - Temperature: 0.6475571437820792\n",
      "EPISODE: 129 - FINAL SCORE: -200 - Temperature: 0.6372185335276287\n",
      "EPISODE: 130 - FINAL SCORE: -200 - Temperature: 0.6270449849407389\n",
      "Updating target network...\n",
      "EPISODE: 131 - FINAL SCORE: -200 - Temperature: 0.6170338627200076\n",
      "EPISODE: 132 - FINAL SCORE: -169 - Temperature: 0.6071825736380868\n",
      "EPISODE: 133 - FINAL SCORE: -200 - Temperature: 0.5974885658699464\n",
      "EPISODE: 134 - FINAL SCORE: -161 - Temperature: 0.5879493283318632\n",
      "EPISODE: 135 - FINAL SCORE: -191 - Temperature: 0.5785623900309638\n",
      "EPISODE: 136 - FINAL SCORE: -162 - Temperature: 0.5693253194251511\n",
      "EPISODE: 137 - FINAL SCORE: -163 - Temperature: 0.5602357237932512\n",
      "EPISODE: 138 - FINAL SCORE: -177 - Temperature: 0.5512912486152157\n",
      "EPISODE: 139 - FINAL SCORE: -164 - Temperature: 0.5424895769622193\n",
      "EPISODE: 140 - FINAL SCORE: -167 - Temperature: 0.5338284288964951\n",
      "Updating target network...\n",
      "EPISODE: 141 - FINAL SCORE: -183 - Temperature: 0.5253055608807516\n",
      "EPISODE: 142 - FINAL SCORE: -200 - Temperature: 0.5169187651970193\n",
      "EPISODE: 143 - FINAL SCORE: -169 - Temperature: 0.5086658693747746\n",
      "EPISODE: 144 - FINAL SCORE: -166 - Temperature: 0.5005447356281957\n",
      "EPISODE: 145 - FINAL SCORE: -164 - Temperature: 0.49255326030240054\n",
      "EPISODE: 146 - FINAL SCORE: -164 - Temperature: 0.48468937332852896\n",
      "EPISODE: 147 - FINAL SCORE: -200 - Temperature: 0.476951037687522\n",
      "EPISODE: 148 - FINAL SCORE: -162 - Temperature: 0.4693362488824643\n",
      "EPISODE: 149 - FINAL SCORE: -172 - Temperature: 0.4618430344193489\n",
      "EPISODE: 150 - FINAL SCORE: -161 - Temperature: 0.45446945329613414\n",
      "Updating target network...\n",
      "EPISODE: 151 - FINAL SCORE: -195 - Temperature: 0.44721359549995626\n",
      "EPISODE: 152 - FINAL SCORE: -200 - Temperature: 0.4400735815123699\n",
      "EPISODE: 153 - FINAL SCORE: -200 - Temperature: 0.43304756182248816\n",
      "EPISODE: 154 - FINAL SCORE: -160 - Temperature: 0.4261337164478946\n",
      "EPISODE: 155 - FINAL SCORE: -166 - Temperature: 0.41933025446320543\n",
      "EPISODE: 156 - FINAL SCORE: -158 - Temperature: 0.41263541353615735\n",
      "EPISODE: 157 - FINAL SCORE: -161 - Temperature: 0.40604745947110277\n",
      "EPISODE: 158 - FINAL SCORE: -180 - Temperature: 0.39956468575979265\n",
      "EPISODE: 159 - FINAL SCORE: -162 - Temperature: 0.3931854131393325\n",
      "EPISODE: 160 - FINAL SCORE: -200 - Temperature: 0.3869079891571941\n",
      "Updating target network...\n",
      "EPISODE: 161 - FINAL SCORE: -162 - Temperature: 0.3807307877431742\n",
      "EPISODE: 162 - FINAL SCORE: -160 - Temperature: 0.3746522087881851\n",
      "EPISODE: 163 - FINAL SCORE: -159 - Temperature: 0.36867067772977163\n",
      "EPISODE: 164 - FINAL SCORE: -163 - Temperature: 0.3627846451442445\n",
      "EPISODE: 165 - FINAL SCORE: -159 - Temperature: 0.3569925863453261\n",
      "EPISODE: 166 - FINAL SCORE: -158 - Temperature: 0.351293000989204\n",
      "EPISODE: 167 - FINAL SCORE: -160 - Temperature: 0.34568441268588995\n",
      "EPISODE: 168 - FINAL SCORE: -161 - Temperature: 0.3401653686167835\n",
      "EPISODE: 169 - FINAL SCORE: -162 - Temperature: 0.3347344391583419\n",
      "EPISODE: 170 - FINAL SCORE: -159 - Temperature: 0.32939021751175807\n",
      "Updating target network...\n",
      "EPISODE: 171 - FINAL SCORE: -161 - Temperature: 0.3241313193385511\n",
      "EPISODE: 172 - FINAL SCORE: -162 - Temperature: 0.31895638240197427\n",
      "EPISODE: 173 - FINAL SCORE: -159 - Temperature: 0.31386406621414886\n",
      "EPISODE: 174 - FINAL SCORE: -157 - Temperature: 0.30885305168883137\n",
      "EPISODE: 175 - FINAL SCORE: -161 - Temperature: 0.3039220407997244\n",
      "EPISODE: 176 - FINAL SCORE: -160 - Temperature: 0.2990697562442428\n",
      "EPISODE: 177 - FINAL SCORE: -160 - Temperature: 0.2942949411126484\n",
      "EPISODE: 178 - FINAL SCORE: -166 - Temperature: 0.2895963585624665\n",
      "EPISODE: 179 - FINAL SCORE: -170 - Temperature: 0.284972791498101\n",
      "EPISODE: 180 - FINAL SCORE: -172 - Temperature: 0.28042304225556447\n",
      "Updating target network...\n",
      "EPISODE: 181 - FINAL SCORE: -170 - Temperature: 0.27594593229224174\n",
      "EPISODE: 182 - FINAL SCORE: -170 - Temperature: 0.27154030188160644\n",
      "EPISODE: 183 - FINAL SCORE: -200 - Temperature: 0.2672050098128118\n",
      "EPISODE: 184 - FINAL SCORE: -173 - Temperature: 0.26293893309507754\n",
      "EPISODE: 185 - FINAL SCORE: -169 - Temperature: 0.258740966666796\n",
      "EPISODE: 186 - FINAL SCORE: -200 - Temperature: 0.25461002310928355\n",
      "EPISODE: 187 - FINAL SCORE: -200 - Temperature: 0.2505450323651009\n",
      "EPISODE: 188 - FINAL SCORE: -200 - Temperature: 0.24654494146087153\n",
      "EPISODE: 189 - FINAL SCORE: -200 - Temperature: 0.24260871423452496\n",
      "EPISODE: 190 - FINAL SCORE: -200 - Temperature: 0.2387353310668949\n",
      "Updating target network...\n",
      "EPISODE: 191 - FINAL SCORE: -200 - Temperature: 0.23492378861760269\n",
      "EPISODE: 192 - FINAL SCORE: -161 - Temperature: 0.23117309956515725\n",
      "EPISODE: 193 - FINAL SCORE: -200 - Temperature: 0.22748229235120465\n",
      "EPISODE: 194 - FINAL SCORE: -200 - Temperature: 0.223850410928861\n",
      "EPISODE: 195 - FINAL SCORE: -200 - Temperature: 0.22027651451506297\n",
      "EPISODE: 196 - FINAL SCORE: -200 - Temperature: 0.21675967734687257\n",
      "EPISODE: 197 - FINAL SCORE: -200 - Temperature: 0.21329898844167244\n",
      "EPISODE: 198 - FINAL SCORE: -200 - Temperature: 0.20989355136118967\n",
      "EPISODE: 199 - FINAL SCORE: -200 - Temperature: 0.2065424839792876\n",
      "EPISODE: 200 - FINAL SCORE: -173 - Temperature: 0.20324491825346414\n",
      "Updating target network...\n",
      "EPISODE: 201 - FINAL SCORE: -200 - Temperature: 0.199999999999999\n",
      "EPISODE: 202 - FINAL SCORE: -158 - Temperature: 0.19680688867269053\n",
      "EPISODE: 203 - FINAL SCORE: -162 - Temperature: 0.193664757145125\n",
      "EPISODE: 204 - FINAL SCORE: -152 - Temperature: 0.19057279149642226\n",
      "EPISODE: 205 - FINAL SCORE: -155 - Temperature: 0.18753019080040212\n",
      "EPISODE: 206 - FINAL SCORE: -156 - Temperature: 0.18453616691811672\n",
      "EPISODE: 207 - FINAL SCORE: -166 - Temperature: 0.18158994429369507\n",
      "EPISODE: 208 - FINAL SCORE: -151 - Temperature: 0.1786907597534475\n",
      "EPISODE: 209 - FINAL SCORE: -157 - Temperature: 0.17583786230817705\n",
      "EPISODE: 210 - FINAL SCORE: -160 - Temperature: 0.1730305129586473\n",
      "Updating target network...\n",
      "EPISODE: 211 - FINAL SCORE: -154 - Temperature: 0.17026798450415603\n",
      "EPISODE: 212 - FINAL SCORE: -160 - Temperature: 0.167549561354165\n",
      "EPISODE: 213 - FINAL SCORE: -200 - Temperature: 0.16487453934293725\n",
      "EPISODE: 214 - FINAL SCORE: -161 - Temperature: 0.16224222554713372\n",
      "EPISODE: 215 - FINAL SCORE: -162 - Temperature: 0.15965193810632228\n",
      "EPISODE: 216 - FINAL SCORE: -158 - Temperature: 0.15710300604635202\n",
      "EPISODE: 217 - FINAL SCORE: -162 - Temperature: 0.15459476910554792\n",
      "EPISODE: 218 - FINAL SCORE: -157 - Temperature: 0.15212657756368012\n",
      "EPISODE: 219 - FINAL SCORE: -163 - Temperature: 0.1496977920736638\n",
      "EPISODE: 220 - FINAL SCORE: -163 - Temperature: 0.14730778349594637\n",
      "Updating target network...\n",
      "EPISODE: 221 - FINAL SCORE: -163 - Temperature: 0.1449559327355383\n",
      "EPISODE: 222 - FINAL SCORE: -200 - Temperature: 0.14264163058164625\n",
      "EPISODE: 223 - FINAL SCORE: -200 - Temperature: 0.14036427754986622\n",
      "EPISODE: 224 - FINAL SCORE: -200 - Temperature: 0.13812328372689645\n",
      "EPISODE: 225 - FINAL SCORE: -162 - Temperature: 0.13591806861772948\n",
      "EPISODE: 226 - FINAL SCORE: -159 - Temperature: 0.13374806099528366\n",
      "EPISODE: 227 - FINAL SCORE: -157 - Temperature: 0.13161269875243573\n",
      "EPISODE: 228 - FINAL SCORE: -164 - Temperature: 0.12951142875641553\n",
      "EPISODE: 229 - FINAL SCORE: -158 - Temperature: 0.12744370670552543\n",
      "EPISODE: 230 - FINAL SCORE: -158 - Temperature: 0.12540899698814748\n",
      "Updating target network...\n",
      "EPISODE: 231 - FINAL SCORE: -159 - Temperature: 0.12340677254400122\n",
      "EPISODE: 232 - FINAL SCORE: -159 - Temperature: 0.12143651472761707\n",
      "EPISODE: 233 - FINAL SCORE: -158 - Temperature: 0.11949771317398897\n",
      "EPISODE: 234 - FINAL SCORE: -163 - Temperature: 0.11758986566637235\n",
      "EPISODE: 235 - FINAL SCORE: -200 - Temperature: 0.11571247800619246\n",
      "EPISODE: 236 - FINAL SCORE: -165 - Temperature: 0.11386506388502993\n",
      "EPISODE: 237 - FINAL SCORE: -200 - Temperature: 0.11204714475864996\n",
      "EPISODE: 238 - FINAL SCORE: -163 - Temperature: 0.11025824972304288\n",
      "EPISODE: 239 - FINAL SCORE: -164 - Temperature: 0.10849791539244358\n",
      "EPISODE: 240 - FINAL SCORE: -172 - Temperature: 0.10676568577929875\n",
      "Updating target network...\n",
      "EPISODE: 241 - FINAL SCORE: -167 - Temperature: 0.10506111217615006\n",
      "EPISODE: 242 - FINAL SCORE: -168 - Temperature: 0.10338375303940359\n",
      "EPISODE: 243 - FINAL SCORE: -162 - Temperature: 0.10173317387495469\n",
      "EPISODE: 244 - FINAL SCORE: -158 - Temperature: 0.10010894712563888\n",
      "EPISODE: 245 - FINAL SCORE: -166 - Temperature: 0.09851065206047988\n",
      "EPISODE: 246 - FINAL SCORE: -159 - Temperature: 0.09693787466570555\n",
      "EPISODE: 247 - FINAL SCORE: -166 - Temperature: 0.09539020753750417\n",
      "EPISODE: 248 - FINAL SCORE: -168 - Temperature: 0.0938672497764926\n",
      "EPISODE: 249 - FINAL SCORE: -200 - Temperature: 0.09236860688386955\n",
      "EPISODE: 250 - FINAL SCORE: -167 - Temperature: 0.09089389065922661\n",
      "Updating target network...\n",
      "EPISODE: 251 - FINAL SCORE: -164 - Temperature: 0.08944271909999102\n",
      "EPISODE: 252 - FINAL SCORE: -161 - Temperature: 0.08801471630247376\n",
      "EPISODE: 253 - FINAL SCORE: -168 - Temperature: 0.08660951236449742\n",
      "EPISODE: 254 - FINAL SCORE: -164 - Temperature: 0.0852267432895787\n",
      "EPISODE: 255 - FINAL SCORE: -158 - Temperature: 0.08386605089264088\n",
      "EPISODE: 256 - FINAL SCORE: -160 - Temperature: 0.08252708270723128\n",
      "EPISODE: 257 - FINAL SCORE: -165 - Temperature: 0.08120949189422035\n",
      "EPISODE: 258 - FINAL SCORE: -200 - Temperature: 0.07991293715195832\n",
      "EPISODE: 259 - FINAL SCORE: -160 - Temperature: 0.0786370826278663\n",
      "EPISODE: 260 - FINAL SCORE: -165 - Temperature: 0.07738159783143862\n",
      "Updating target network...\n",
      "EPISODE: 261 - FINAL SCORE: -163 - Temperature: 0.07614615754863464\n",
      "EPISODE: 262 - FINAL SCORE: -153 - Temperature: 0.07493044175763684\n",
      "EPISODE: 263 - FINAL SCORE: -158 - Temperature: 0.07373413554595415\n",
      "EPISODE: 264 - FINAL SCORE: -156 - Temperature: 0.07255692902884871\n",
      "EPISODE: 265 - FINAL SCORE: -153 - Temperature: 0.07139851726906504\n",
      "EPISODE: 266 - FINAL SCORE: -155 - Temperature: 0.07025860019784064\n",
      "EPISODE: 267 - FINAL SCORE: -155 - Temperature: 0.06913688253717781\n",
      "EPISODE: 268 - FINAL SCORE: -153 - Temperature: 0.06803307372335653\n",
      "EPISODE: 269 - FINAL SCORE: -156 - Temperature: 0.06694688783166822\n",
      "EPISODE: 270 - FINAL SCORE: -155 - Temperature: 0.06587804350235146\n",
      "Updating target network...\n",
      "EPISODE: 271 - FINAL SCORE: -159 - Temperature: 0.06482626386771007\n",
      "EPISODE: 272 - FINAL SCORE: -158 - Temperature: 0.06379127648039469\n",
      "EPISODE: 273 - FINAL SCORE: -152 - Temperature: 0.06277281324282961\n",
      "EPISODE: 274 - FINAL SCORE: -155 - Temperature: 0.06177061033776612\n",
      "EPISODE: 275 - FINAL SCORE: -156 - Temperature: 0.060784408159944715\n",
      "EPISODE: 276 - FINAL SCORE: -159 - Temperature: 0.059813951248848404\n",
      "EPISODE: 277 - FINAL SCORE: -156 - Temperature: 0.05885898822252954\n",
      "EPISODE: 278 - FINAL SCORE: -152 - Temperature: 0.057919271712493156\n",
      "EPISODE: 279 - FINAL SCORE: -160 - Temperature: 0.05699455829962005\n",
      "EPISODE: 280 - FINAL SCORE: -160 - Temperature: 0.05608460845111275\n",
      "Updating target network...\n",
      "EPISODE: 281 - FINAL SCORE: -152 - Temperature: 0.055189186458448204\n",
      "EPISODE: 282 - FINAL SCORE: -156 - Temperature: 0.05430806037632115\n",
      "EPISODE: 283 - FINAL SCORE: -156 - Temperature: 0.05344100196256223\n",
      "EPISODE: 284 - FINAL SCORE: -152 - Temperature: 0.052587786619015364\n",
      "EPISODE: 285 - FINAL SCORE: -158 - Temperature: 0.051748193333359074\n",
      "EPISODE: 286 - FINAL SCORE: -155 - Temperature: 0.05092200462185659\n",
      "EPISODE: 287 - FINAL SCORE: -151 - Temperature: 0.05010900647302006\n",
      "EPISODE: 288 - FINAL SCORE: -156 - Temperature: 0.049308988292174186\n",
      "EPISODE: 289 - FINAL SCORE: -153 - Temperature: 0.04852174284690487\n",
      "EPISODE: 290 - FINAL SCORE: -153 - Temperature: 0.04774706621337886\n",
      "Updating target network...\n",
      "EPISODE: 291 - FINAL SCORE: -152 - Temperature: 0.046984757723520426\n",
      "EPISODE: 292 - FINAL SCORE: -200 - Temperature: 0.04623461991303133\n",
      "EPISODE: 293 - FINAL SCORE: -151 - Temperature: 0.045496458470240815\n",
      "EPISODE: 294 - FINAL SCORE: -150 - Temperature: 0.04477008218577209\n",
      "EPISODE: 295 - FINAL SCORE: -152 - Temperature: 0.04405530290301249\n",
      "EPISODE: 296 - FINAL SCORE: -154 - Temperature: 0.04335193546937441\n",
      "EPISODE: 297 - FINAL SCORE: -200 - Temperature: 0.042659797688334375\n",
      "EPISODE: 298 - FINAL SCORE: -161 - Temperature: 0.04197871027223784\n",
      "EPISODE: 299 - FINAL SCORE: -153 - Temperature: 0.041308496795857415\n",
      "EPISODE: 300 - FINAL SCORE: -158 - Temperature: 0.04064898365069272\n",
      "Updating target network...\n",
      "EPISODE: 301 - FINAL SCORE: -159 - Temperature: 0.039999999999999696\n",
      "EPISODE: 302 - FINAL SCORE: -152 - Temperature: 0.03936137773453801\n",
      "EPISODE: 303 - FINAL SCORE: -148 - Temperature: 0.0387329514290249\n",
      "EPISODE: 304 - FINAL SCORE: -154 - Temperature: 0.03811455829928436\n",
      "EPISODE: 305 - FINAL SCORE: -152 - Temperature: 0.037506038160080336\n",
      "EPISODE: 306 - FINAL SCORE: -156 - Temperature: 0.03690723338362325\n",
      "EPISODE: 307 - FINAL SCORE: -150 - Temperature: 0.03631798885873892\n",
      "EPISODE: 308 - FINAL SCORE: -154 - Temperature: 0.03573815195068941\n",
      "EPISODE: 309 - FINAL SCORE: -158 - Temperature: 0.03516757246163532\n",
      "EPISODE: 310 - FINAL SCORE: -148 - Temperature: 0.034606102591729374\n",
      "Updating target network...\n",
      "EPISODE: 311 - FINAL SCORE: -146 - Temperature: 0.034053596900831117\n",
      "EPISODE: 312 - FINAL SCORE: -148 - Temperature: 0.033509912270832914\n",
      "EPISODE: 313 - FINAL SCORE: -154 - Temperature: 0.032974907868587366\n",
      "EPISODE: 314 - FINAL SCORE: -150 - Temperature: 0.03244844510942666\n",
      "EPISODE: 315 - FINAL SCORE: -152 - Temperature: 0.03193038762126438\n",
      "EPISODE: 316 - FINAL SCORE: -150 - Temperature: 0.03142060120927033\n",
      "EPISODE: 317 - FINAL SCORE: -155 - Temperature: 0.03091895382110951\n",
      "EPISODE: 318 - FINAL SCORE: -149 - Temperature: 0.030425315512735942\n",
      "EPISODE: 319 - FINAL SCORE: -151 - Temperature: 0.029939558414732687\n",
      "EPISODE: 320 - FINAL SCORE: -158 - Temperature: 0.0294615566991892\n",
      "Updating target network...\n",
      "EPISODE: 321 - FINAL SCORE: -151 - Temperature: 0.02899118654710759\n",
      "EPISODE: 322 - FINAL SCORE: -152 - Temperature: 0.028528326116329175\n",
      "EPISODE: 323 - FINAL SCORE: -159 - Temperature: 0.028072855509973172\n",
      "EPISODE: 324 - FINAL SCORE: -200 - Temperature: 0.027624656745379225\n",
      "EPISODE: 325 - FINAL SCORE: -156 - Temperature: 0.02718361372354583\n",
      "EPISODE: 326 - FINAL SCORE: -151 - Temperature: 0.026749612199056666\n",
      "EPISODE: 327 - FINAL SCORE: -150 - Temperature: 0.026322539750487078\n",
      "EPISODE: 328 - FINAL SCORE: -155 - Temperature: 0.02590228575128304\n",
      "EPISODE: 329 - FINAL SCORE: -151 - Temperature: 0.02548874134110502\n",
      "EPISODE: 330 - FINAL SCORE: -150 - Temperature: 0.02508179939762943\n",
      "Updating target network...\n",
      "EPISODE: 331 - FINAL SCORE: -161 - Temperature: 0.024681354508800185\n",
      "EPISODE: 332 - FINAL SCORE: -148 - Temperature: 0.02428730294552335\n",
      "EPISODE: 333 - FINAL SCORE: -150 - Temperature: 0.02389954263479773\n",
      "EPISODE: 334 - FINAL SCORE: -151 - Temperature: 0.02351797313327441\n",
      "EPISODE: 335 - FINAL SCORE: -148 - Temperature: 0.023142495601238437\n",
      "EPISODE: 336 - FINAL SCORE: -155 - Temperature: 0.022773012777005927\n",
      "EPISODE: 337 - FINAL SCORE: -153 - Temperature: 0.022409428951729937\n",
      "EPISODE: 338 - FINAL SCORE: -151 - Temperature: 0.022051649944608523\n",
      "EPISODE: 339 - FINAL SCORE: -148 - Temperature: 0.02169958307848866\n",
      "EPISODE: 340 - FINAL SCORE: -151 - Temperature: 0.021353137155859694\n",
      "Updating target network...\n",
      "EPISODE: 341 - FINAL SCORE: -156 - Temperature: 0.02101222243522996\n",
      "EPISODE: 342 - FINAL SCORE: -152 - Temperature: 0.02067675060788067\n",
      "EPISODE: 343 - FINAL SCORE: -151 - Temperature: 0.020346634774990886\n",
      "EPISODE: 344 - FINAL SCORE: -151 - Temperature: 0.020021789425127726\n",
      "EPISODE: 345 - FINAL SCORE: -150 - Temperature: 0.019702130412095925\n",
      "EPISODE: 346 - FINAL SCORE: -148 - Temperature: 0.019387574933141062\n",
      "EPISODE: 347 - FINAL SCORE: -145 - Temperature: 0.01907804150750079\n",
      "EPISODE: 348 - FINAL SCORE: -148 - Temperature: 0.018773449955298477\n",
      "EPISODE: 349 - FINAL SCORE: -147 - Temperature: 0.018473721376773864\n",
      "EPISODE: 350 - FINAL SCORE: -158 - Temperature: 0.018178778131845277\n",
      "Updating target network...\n",
      "EPISODE: 351 - FINAL SCORE: -155 - Temperature: 0.017888543819998163\n",
      "EPISODE: 352 - FINAL SCORE: -146 - Temperature: 0.01760294326049471\n",
      "EPISODE: 353 - FINAL SCORE: -148 - Temperature: 0.01732190247289944\n",
      "EPISODE: 354 - FINAL SCORE: -150 - Temperature: 0.0170453486579157\n",
      "EPISODE: 355 - FINAL SCORE: -156 - Temperature: 0.016773210178528133\n",
      "EPISODE: 356 - FINAL SCORE: -145 - Temperature: 0.016505416541446213\n",
      "EPISODE: 357 - FINAL SCORE: -150 - Temperature: 0.01624189837884403\n",
      "EPISODE: 358 - FINAL SCORE: -157 - Temperature: 0.015982587430391628\n",
      "EPISODE: 359 - FINAL SCORE: -148 - Temperature: 0.015727416525573218\n",
      "EPISODE: 360 - FINAL SCORE: -147 - Temperature: 0.015476319566287686\n",
      "Updating target network...\n",
      "EPISODE: 361 - FINAL SCORE: -155 - Temperature: 0.01522923150972689\n",
      "EPISODE: 362 - FINAL SCORE: -149 - Temperature: 0.01498608835152733\n",
      "EPISODE: 363 - FINAL SCORE: -147 - Temperature: 0.014746827109190792\n",
      "EPISODE: 364 - FINAL SCORE: -146 - Temperature: 0.014511385805769706\n",
      "EPISODE: 365 - FINAL SCORE: -200 - Temperature: 0.014279703453812973\n",
      "EPISODE: 366 - FINAL SCORE: -200 - Temperature: 0.014051720039568092\n",
      "EPISODE: 367 - FINAL SCORE: -149 - Temperature: 0.01382737650743553\n",
      "EPISODE: 368 - FINAL SCORE: -153 - Temperature: 0.01360661474467127\n",
      "EPISODE: 369 - FINAL SCORE: -157 - Temperature: 0.01338937756633361\n",
      "EPISODE: 370 - FINAL SCORE: -200 - Temperature: 0.01317560870047026\n",
      "Updating target network...\n",
      "EPISODE: 371 - FINAL SCORE: -149 - Temperature: 0.01296525277354198\n",
      "EPISODE: 372 - FINAL SCORE: -148 - Temperature: 0.012758255296078908\n",
      "EPISODE: 373 - FINAL SCORE: -150 - Temperature: 0.012554562648565892\n",
      "EPISODE: 374 - FINAL SCORE: -152 - Temperature: 0.012354122067553193\n",
      "EPISODE: 375 - FINAL SCORE: -152 - Temperature: 0.012156881631988914\n",
      "EPISODE: 376 - FINAL SCORE: -152 - Temperature: 0.011962790249769654\n",
      "EPISODE: 377 - FINAL SCORE: -200 - Temperature: 0.011771797644505876\n",
      "EPISODE: 378 - FINAL SCORE: -149 - Temperature: 0.011583854342498603\n",
      "EPISODE: 379 - FINAL SCORE: -151 - Temperature: 0.011398911659923983\n",
      "EPISODE: 380 - FINAL SCORE: -150 - Temperature: 0.011216921690222523\n",
      "Updating target network...\n",
      "EPISODE: 381 - FINAL SCORE: -200 - Temperature: 0.011037837291689615\n",
      "EPISODE: 382 - FINAL SCORE: -200 - Temperature: 0.010861612075264203\n",
      "EPISODE: 383 - FINAL SCORE: -158 - Temperature: 0.01068820039251242\n",
      "EPISODE: 384 - FINAL SCORE: -200 - Temperature: 0.010517557323803047\n",
      "EPISODE: 385 - FINAL SCORE: -154 - Temperature: 0.010349638666671789\n",
      "EPISODE: 386 - FINAL SCORE: -154 - Temperature: 0.010184400924371291\n",
      "EPISODE: 387 - FINAL SCORE: -148 - Temperature: 0.010021801294603987\n",
      "EPISODE: 388 - FINAL SCORE: -148 - Temperature: 0.00986179765843481\n",
      "EPISODE: 389 - FINAL SCORE: -152 - Temperature: 0.00970434856938095\n",
      "EPISODE: 390 - FINAL SCORE: -152 - Temperature: 0.009549413242675749\n",
      "Updating target network...\n",
      "EPISODE: 391 - FINAL SCORE: -154 - Temperature: 0.00939695154470406\n",
      "EPISODE: 392 - FINAL SCORE: -154 - Temperature: 0.009246923982606244\n",
      "EPISODE: 393 - FINAL SCORE: -153 - Temperature: 0.00909929169404814\n",
      "EPISODE: 394 - FINAL SCORE: -156 - Temperature: 0.008954016437154395\n",
      "EPISODE: 395 - FINAL SCORE: -154 - Temperature: 0.008811060580602475\n",
      "EPISODE: 396 - FINAL SCORE: -159 - Temperature: 0.00867038709387486\n",
      "EPISODE: 397 - FINAL SCORE: -153 - Temperature: 0.008531959537666854\n",
      "EPISODE: 398 - FINAL SCORE: -200 - Temperature: 0.008395742054447545\n",
      "EPISODE: 399 - FINAL SCORE: -150 - Temperature: 0.008261699359171462\n",
      "EPISODE: 400 - FINAL SCORE: -150 - Temperature: 0.008129796730138524\n",
      "Updating target network...\n",
      "EPISODE: 401 - FINAL SCORE: -160 - Temperature: 0.00799999999999992\n",
      "EPISODE: 402 - FINAL SCORE: -155 - Temperature: 0.007872275546907581\n",
      "EPISODE: 403 - FINAL SCORE: -160 - Temperature: 0.007746590285804961\n",
      "EPISODE: 404 - FINAL SCORE: -200 - Temperature: 0.007622911659856853\n",
      "EPISODE: 405 - FINAL SCORE: -148 - Temperature: 0.007501207632016048\n",
      "EPISODE: 406 - FINAL SCORE: -157 - Temperature: 0.007381446676724631\n",
      "EPISODE: 407 - FINAL SCORE: -149 - Temperature: 0.0072635977717477665\n",
      "EPISODE: 408 - FINAL SCORE: -155 - Temperature: 0.007147630390137864\n",
      "EPISODE: 409 - FINAL SCORE: -164 - Temperature: 0.007033514492327047\n",
      "EPISODE: 410 - FINAL SCORE: -152 - Temperature: 0.006921220518345858\n",
      "Updating target network...\n",
      "EPISODE: 411 - FINAL SCORE: -157 - Temperature: 0.006810719380166207\n",
      "EPISODE: 412 - FINAL SCORE: -154 - Temperature: 0.006701982454166566\n",
      "EPISODE: 413 - FINAL SCORE: -159 - Temperature: 0.006594981573717456\n",
      "EPISODE: 414 - FINAL SCORE: -153 - Temperature: 0.006489689021885316\n",
      "EPISODE: 415 - FINAL SCORE: -159 - Temperature: 0.006386077524252859\n",
      "EPISODE: 416 - FINAL SCORE: -156 - Temperature: 0.00628412024185405\n",
      "EPISODE: 417 - FINAL SCORE: -156 - Temperature: 0.006183790764221886\n",
      "EPISODE: 418 - FINAL SCORE: -157 - Temperature: 0.006085063102547174\n",
      "EPISODE: 419 - FINAL SCORE: -153 - Temperature: 0.005987911682946522\n",
      "EPISODE: 420 - FINAL SCORE: -152 - Temperature: 0.005892311339837825\n",
      "Updating target network...\n",
      "EPISODE: 421 - FINAL SCORE: -152 - Temperature: 0.0057982373094215035\n",
      "EPISODE: 422 - FINAL SCORE: -148 - Temperature: 0.005705665223265821\n",
      "EPISODE: 423 - FINAL SCORE: -155 - Temperature: 0.00561457110199462\n",
      "EPISODE: 424 - FINAL SCORE: -149 - Temperature: 0.005524931349075831\n",
      "EPISODE: 425 - FINAL SCORE: -152 - Temperature: 0.005436722744709152\n",
      "EPISODE: 426 - FINAL SCORE: -151 - Temperature: 0.005349922439811319\n",
      "EPISODE: 427 - FINAL SCORE: -153 - Temperature: 0.0052645079500974025\n",
      "EPISODE: 428 - FINAL SCORE: -154 - Temperature: 0.005180457150256595\n",
      "EPISODE: 429 - FINAL SCORE: -150 - Temperature: 0.005097748268220992\n",
      "EPISODE: 430 - FINAL SCORE: -152 - Temperature: 0.005016359879525873\n",
      "Updating target network...\n",
      "EPISODE: 431 - FINAL SCORE: -150 - Temperature: 0.004936270901760024\n",
      "EPISODE: 432 - FINAL SCORE: -154 - Temperature: 0.004857460589104658\n",
      "EPISODE: 433 - FINAL SCORE: -155 - Temperature: 0.004779908526959534\n",
      "EPISODE: 434 - FINAL SCORE: -153 - Temperature: 0.0047035946266548705\n",
      "EPISODE: 435 - FINAL SCORE: -154 - Temperature: 0.004628499120247676\n",
      "EPISODE: 436 - FINAL SCORE: -152 - Temperature: 0.004554602555401174\n",
      "EPISODE: 437 - FINAL SCORE: -152 - Temperature: 0.004481885790345977\n",
      "EPISODE: 438 - FINAL SCORE: -152 - Temperature: 0.004410329988921693\n",
      "EPISODE: 439 - FINAL SCORE: -150 - Temperature: 0.004339916615697722\n",
      "EPISODE: 440 - FINAL SCORE: -154 - Temperature: 0.004270627431171929\n",
      "Updating target network...\n",
      "EPISODE: 441 - FINAL SCORE: -152 - Temperature: 0.004202444487045981\n",
      "EPISODE: 442 - FINAL SCORE: -152 - Temperature: 0.004135350121576124\n",
      "EPISODE: 443 - FINAL SCORE: -158 - Temperature: 0.004069326954998167\n",
      "EPISODE: 444 - FINAL SCORE: -153 - Temperature: 0.0040043578850255356\n",
      "EPISODE: 445 - FINAL SCORE: -153 - Temperature: 0.003940426082419175\n",
      "EPISODE: 446 - FINAL SCORE: -157 - Temperature: 0.0038775149866282026\n",
      "EPISODE: 447 - FINAL SCORE: -153 - Temperature: 0.0038156083015001476\n",
      "EPISODE: 448 - FINAL SCORE: -152 - Temperature: 0.003754689991059686\n",
      "EPISODE: 449 - FINAL SCORE: -153 - Temperature: 0.003694744275354764\n",
      "EPISODE: 450 - FINAL SCORE: -156 - Temperature: 0.003635755626369046\n",
      "Updating target network...\n",
      "EPISODE: 451 - FINAL SCORE: -153 - Temperature: 0.0035777087639996237\n",
      "EPISODE: 452 - FINAL SCORE: -157 - Temperature: 0.003520588652098933\n",
      "EPISODE: 453 - FINAL SCORE: -153 - Temperature: 0.003464380494579879\n",
      "EPISODE: 454 - FINAL SCORE: -150 - Temperature: 0.0034090697315831316\n",
      "EPISODE: 455 - FINAL SCORE: -152 - Temperature: 0.0033546420357056183\n",
      "EPISODE: 456 - FINAL SCORE: -152 - Temperature: 0.003301083308289234\n",
      "EPISODE: 457 - FINAL SCORE: -159 - Temperature: 0.0032483796757687977\n",
      "EPISODE: 458 - FINAL SCORE: -151 - Temperature: 0.0031965174860783176\n",
      "EPISODE: 459 - FINAL SCORE: -154 - Temperature: 0.0031454833051146364\n",
      "EPISODE: 460 - FINAL SCORE: -156 - Temperature: 0.0030952639132575295\n",
      "Updating target network...\n",
      "EPISODE: 461 - FINAL SCORE: -156 - Temperature: 0.0030458463019453707\n",
      "EPISODE: 462 - FINAL SCORE: -200 - Temperature: 0.0029972176703054586\n",
      "EPISODE: 463 - FINAL SCORE: -155 - Temperature: 0.002949365421838151\n",
      "EPISODE: 464 - FINAL SCORE: -154 - Temperature: 0.0029022771611539344\n",
      "EPISODE: 465 - FINAL SCORE: -156 - Temperature: 0.0028559406907625873\n",
      "EPISODE: 466 - FINAL SCORE: -154 - Temperature: 0.0028103440079136114\n",
      "EPISODE: 467 - FINAL SCORE: -154 - Temperature: 0.002765475301487099\n",
      "EPISODE: 468 - FINAL SCORE: -156 - Temperature: 0.0027213229489342476\n",
      "EPISODE: 469 - FINAL SCORE: -154 - Temperature: 0.0026778755132667153\n",
      "EPISODE: 470 - FINAL SCORE: -156 - Temperature: 0.0026351217400940453\n",
      "Updating target network...\n",
      "EPISODE: 471 - FINAL SCORE: -151 - Temperature: 0.0025930505547083894\n",
      "EPISODE: 472 - FINAL SCORE: -200 - Temperature: 0.002551651059215775\n",
      "EPISODE: 473 - FINAL SCORE: -155 - Temperature: 0.002510912529713172\n",
      "EPISODE: 474 - FINAL SCORE: -152 - Temperature: 0.002470824413510633\n",
      "EPISODE: 475 - FINAL SCORE: -200 - Temperature: 0.0024313763263977767\n",
      "EPISODE: 476 - FINAL SCORE: -156 - Temperature: 0.0023925580499539246\n",
      "EPISODE: 477 - FINAL SCORE: -158 - Temperature: 0.0023543595289011697\n",
      "EPISODE: 478 - FINAL SCORE: -151 - Temperature: 0.002316770868499715\n",
      "EPISODE: 479 - FINAL SCORE: -152 - Temperature: 0.002279782331984791\n",
      "EPISODE: 480 - FINAL SCORE: -152 - Temperature: 0.002243384338044499\n",
      "Updating target network...\n",
      "EPISODE: 481 - FINAL SCORE: -153 - Temperature: 0.0022075674583379172\n",
      "EPISODE: 482 - FINAL SCORE: -152 - Temperature: 0.0021723224150528354\n",
      "EPISODE: 483 - FINAL SCORE: -152 - Temperature: 0.0021376400785024785\n",
      "EPISODE: 484 - FINAL SCORE: -150 - Temperature: 0.0021035114647606043\n",
      "EPISODE: 485 - FINAL SCORE: -152 - Temperature: 0.0020699277333343525\n",
      "EPISODE: 486 - FINAL SCORE: -153 - Temperature: 0.002036880184874253\n",
      "EPISODE: 487 - FINAL SCORE: -154 - Temperature: 0.002004360258920792\n",
      "EPISODE: 488 - FINAL SCORE: -150 - Temperature: 0.0019723595316869574\n",
      "EPISODE: 489 - FINAL SCORE: -152 - Temperature: 0.0019408697138761852\n",
      "EPISODE: 490 - FINAL SCORE: -152 - Temperature: 0.0019098826485351448\n",
      "Updating target network...\n",
      "EPISODE: 491 - FINAL SCORE: -150 - Temperature: 0.0018793903089408075\n",
      "EPISODE: 492 - FINAL SCORE: -150 - Temperature: 0.001849384796521244\n",
      "EPISODE: 493 - FINAL SCORE: -152 - Temperature: 0.0018198583388096237\n",
      "EPISODE: 494 - FINAL SCORE: -153 - Temperature: 0.0017908032874308748\n",
      "EPISODE: 495 - FINAL SCORE: -152 - Temperature: 0.0017622121161204908\n",
      "EPISODE: 496 - FINAL SCORE: -151 - Temperature: 0.0017340774187749675\n",
      "EPISODE: 497 - FINAL SCORE: -152 - Temperature: 0.0017063919075333664\n",
      "EPISODE: 498 - FINAL SCORE: -158 - Temperature: 0.0016791484108895048\n",
      "EPISODE: 499 - FINAL SCORE: -151 - Temperature: 0.0016523398718342884\n",
      "EPISODE: 500 - FINAL SCORE: -155 - Temperature: 0.0016259593460277006\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Gym environment\n",
    "env = gym.make('MountainCar-v0') \n",
    "env.seed(0) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "for episode_num, tau in enumerate(exploration_profile):\n",
    "\n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "    score = 0\n",
    "    done = False\n",
    "    \n",
    "    x0 = np.abs(state[0]+0.5)\n",
    "    xm = x0\n",
    "    v0 = np.abs(state[1])\n",
    "    vm = v0\n",
    "    cc = 0\n",
    "\n",
    "    # Go on until the pole falls off\n",
    "    while not done:\n",
    "        cc += 1\n",
    "\n",
    "        # Choose the action following the policy\n",
    "        action, q_values = choose_action_softmax(policy_net, state, temperature=tau)\n",
    "\n",
    "        # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # We apply a (linear) penalty when the cart is far from center\n",
    "        xc = np.abs(next_state[0]+0.5)\n",
    "        vc = np.abs(next_state[1])\n",
    "        if (xc >= xm):\n",
    "            xm = xc\n",
    "            reward += 20\n",
    "        if (vc >= vm):\n",
    "            vm = vc\n",
    "            reward += 20\n",
    "        # reward += np.abs(state[1])\n",
    "        # increment the maxes by steps\n",
    "        #pos_weight = 10\n",
    "        #reward = np.abs(next_state[0]+0.5)\n",
    "        if (next_state[0] >=0.5): reward += 2100 - cc*10\n",
    "        # Update the final score (+1 for each step)\n",
    "        score += -1\n",
    "\n",
    "        # Apply penalty for bad state\n",
    "        if done: # if the pole has fallen down \n",
    "            reward += bad_state_penalty\n",
    "            next_state = None\n",
    "\n",
    "        # Update the replay memory\n",
    "        replay_mem.push(state, action, next_state, reward)\n",
    "\n",
    "        # Update the network\n",
    "        if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "            update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "\n",
    "        # Visually render the environment (disable to speed up the training)\n",
    "        #env.render()\n",
    "\n",
    "        # Set the current state for the next iteration\n",
    "        state = next_state\n",
    "\n",
    "    # Update the target network every target_net_update_steps episodes\n",
    "    if episode_num % target_net_update_steps == 0:\n",
    "        print('Updating target network...')\n",
    "        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "\n",
    "    # Print the final score\n",
    "    print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}\") # Print the final score\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkG9iDZTIhzc"
   },
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - FINAL SCORE: -153.0\n",
      "EPISODE 2 - FINAL SCORE: -158.0\n",
      "EPISODE 3 - FINAL SCORE: -162.0\n",
      "EPISODE 4 - FINAL SCORE: -158.0\n",
      "EPISODE 5 - FINAL SCORE: -153.0\n",
      "EPISODE 6 - FINAL SCORE: -152.0\n",
      "EPISODE 7 - FINAL SCORE: -156.0\n",
      "EPISODE 8 - FINAL SCORE: -152.0\n",
      "EPISODE 9 - FINAL SCORE: -153.0\n",
      "EPISODE 10 - FINAL SCORE: -200.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Gym environment\n",
    "env = gym.make('MountainCar-v0') \n",
    "env.seed(1) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# Let's try for a total of 10 episodes\n",
    "for num_episode in range(10): \n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    # Reset the score. \n",
    "    score = 0\n",
    "    done = False\n",
    "    # Go on until the pole falls off or the score reach 490\n",
    "    while not done:\n",
    "      # Choose the best action (temperature 0)\n",
    "      action, q_values = choose_action_softmax(policy_net, state, temperature=0)\n",
    "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      # Visually render the environment\n",
    "      env.render()\n",
    "      # Update the final score (-1 for each step)\n",
    "      score += reward \n",
    "      # Set the current state for the next iteration\n",
    "      state = next_state\n",
    "    # Print the final score\n",
    "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the agent is able to beat the game 9 time out 10, on average within the 160 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nndl_2021_lab_07_deep_reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
